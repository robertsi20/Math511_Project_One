# -*- coding: utf-8 -*-
"""Testing Speed of LSH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rh7vafQNcAr4ddkQhhyrSmIjJ6AAp42R
"""

import requests
import pandas as pd
import numpy as np
import io
from tqdm import tqdm
from itertools import combinations

#Getting dataset

url = "https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/sick2014/SICK_train.txt"

text = requests.get(url).text

data = pd.read_csv(io.StringIO(text), sep='\t')
data.head()

#Creating large list of sentences
sentences = data['sentence_A'].tolist()+data['sentence_B'].tolist()

#Function to check jaccard similarity
def jaccard_sim(set1,set2):
  return len(set1.intersection(set2))/len(set1.union(set2)) 

#Function to brute force similarity of each pair of points in the dataset without using LSH
def pairwise_jsim(shingles,sim_thresh):
  pairs=[]
  for i in tqdm(range(len(shingles))):
    for j in range(i+1,len(shingles)):
      if jaccard_sim(shingles[i],shingles[j])>=sim_thresh:
        pairs.append((i,j))
  return pairs, len(pairs)

#Function to create shingles
def build_shingles(sentence: str, k: int):
    shingles = []
    for i in range(len(sentence) - k):
        shingles.append(sentence[i:i+k])
    return set(shingles)

#Functions for LSH process
def build_vocab(shingle_sets: list):
    # convert list of shingle sets into single set
    full_set = {item for set_ in shingle_sets for item in set_}
    vocab = {}
    print(full_set)
    for i, shingle in enumerate(list(full_set)):
        vocab[shingle] = i
    return vocab

def one_hot(shingles: set, vocab: dict):
    vec = np.zeros(len(vocab))
    for shingle in shingles:
        idx = vocab[shingle]
        vec[idx] = 1
    return vec

def minhash_arr(vocab: dict, resolution: int):
    length = len(vocab.keys())
    arr = np.zeros((resolution, length))
    for i in range(resolution):
        permutation = np.random.permutation(len(vocab)) + 1
        arr[i, :] = permutation.copy()
    return arr.astype(int)

def get_signature(minhash, vector):
    # get index locations of every 1 value in vector
    idx = np.nonzero(vector)[0].tolist()
    # use index locations to pull only +ve positions in minhash
    shingles = minhash[:, idx]
    # find minimum value in each hash vector
    signature = np.min(shingles, axis=1)
    return signature

class LSH:
    buckets = []
    counter = 0
    def __init__(self, b):
        self.b = b
        for i in range(b):
            self.buckets.append({})

    def make_subvecs(self, signature):
        l = len(signature)
        assert l % self.b == 0
        r = int(l / self.b)
        # break signature into subvectors
        subvecs = []
        for i in range(0, l, r):
            subvecs.append(signature[i:i+r])
            #print(np.stack(subvecs))
        return np.stack(subvecs)
        
    
    def add_hash(self, signature):
        subvecs = self.make_subvecs(signature).astype(str)
        #print(subvecs)
        for i, subvec in enumerate(subvecs):
            subvec = ','.join(subvec)
            if subvec not in self.buckets[i].keys():
                self.buckets[i][subvec] = []
            self.buckets[i][subvec].append(self.counter)
        self.counter += 1
        #print(self.counter)

    def check_candidates(self):
        candidates = []
        for bucket_band in self.buckets:
            #print(bucket_band)
            keys = bucket_band.keys()
            for bucket in keys:
                hits = bucket_band[bucket]
                if len(hits) > 1:
                    candidates.extend(combinations(hits, 2))
        return set(candidates)

#Creating shingles for each sentence in the dataset

k = 6 

shingles = []
for sentence in sentences:
    shingles.append(build_shingles(sentence, k))

#Brute forcing the jaccard similarity of each pair of shingled sentences for different numbers of sentences

small=shingles[0:2250]

med=shingles[0:4500]

large=shingles[0:9000]

smcands,smcount=pairwise_jsim(small,.6)

mcands,mcount=pairwise_jsim(med,.6)

lcands,lcount=pairwise_jsim(large,.6)

#Running the entire LSH process to check runtime

#Building vocab for one hot encoding
vocab = build_vocab(shingles)

# one hot encoding
shingles_1hot = []
for shingle_set in shingles:
    shingles_1hot.append(one_hot(shingle_set, vocab))

# stacking into numpy
shingles_1hot = np.stack(shingles_1hot)
shingles_1hot.shape

#Creating minhash array
arr = minhash_arr(vocab, 100)
signatures = []

#Creating signatures
for vector in shingles_1hot:
    signatures.append(get_signature(arr, vector))

# merging signatures into single array
signatures = np.stack(signatures)
signatures.shape

#Creating LSH object and correct number of buckets
b = 20
lsh = LSH(b)

#Filling buckets
for signature in signatures:
    lsh.add_hash(signature)
lsh.buckets

#Checking for candidate pairs
candidate_pairs = lsh.check_candidates()
print(len(candidate_pairs))

#Runtime is about 6 seconds for the entire process

#Finding the jaccard similarity of the 36000 candidate pairs generated from LSH
candidate_pairs=list(candidate_pairs)
LSHsims=[]
for i in tqdm(candidate_pairs):
  LSHsims.append(jaccard_sim(shingles[i[0]],shingles[i[1]]))

#Finding all pairs from 36049 that are greater than .6
finLSHpairs=[]
for i in range(len(LSHsims)):
  if LSHsims[i]>.6:
    finLSHpairs.append(candidate_pairs[i])

#Checking counts of pairs with jaccard similarity >.6 from brute force method and LSH method
print(len(finLSHpairs))
print(lcount)

#Checking the intersection of the LSH method pairs and the brute force method pairs

print(len(set(finLSHpairs).intersection(set(lcands))))

#Here, we can see that the intersection between the pairs with jaccard similarity greater than .6 found using the brute force method and the pairs with pairs with jaccard similarity greater than
#.6 using the LSH method is 18429, which is the same as the size of the pairs using the LSH method. Therefore, the LSH method is 32 times faster and only misses about 800 pairs out of 19307